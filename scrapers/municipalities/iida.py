import sys
sys.path.append('../../')

from scrapers.base_scraper import BaseScraper
from bs4 import BeautifulSoup
import re

class IidaScraper(BaseScraper):
    """é•·é‡çœŒé£¯ç”°å¸‚ã®ç©ºãå®¶ãƒãƒ³ã‚¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self):
        super().__init__(
            municipality_name="é•·é‡çœŒé£¯ç”°å¸‚",
            base_url="https://www.city.iida.lg.jp/site/akiyabank/"
        )
    
    def parse_detail_page(self, url, property_id):
        """ç‰©ä»¶è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’å–å¾—"""
        print(f"\n  è©³ç´°ãƒšãƒ¼ã‚¸å–å¾—: {property_id}")
        html = self.fetch_page(url)
        
        if not html:
            return {}
        
        soup = BeautifulSoup(html, 'html.parser')
        
        detail_data = {
            'price': None,
            'price_raw': None,
            'land_area': None,
            'building_area': None,
            'built_year': None,
            'description': '',
            'images': []
        }
        
        page_text = soup.get_text()
        
        for line in page_text.split('\n'):
            line = line.strip()
            
            if ('ä¾¡æ ¼' in line or 'é‡‘é¡' in line or 'å£²è²·' in line) and 'ä¸‡å††' in line:
                detail_data['price_raw'] = line
                price_match = re.search(r'(\d+)ä¸‡å††', line)
                if price_match:
                    detail_data['price'] = int(price_match.group(1)) * 10000
                print(f"    ğŸ’° {line}")
            
            if 'åœŸåœ°' in line and ('ã¡' in line or 'åª' in line or 'å¹³ç±³' in line):
                detail_data['land_area'] = line
                print(f"    ğŸ“ {line}")
            
            if 'å»ºç‰©' in line and ('ã¡' in line or 'åª' in line or 'å¹³ç±³' in line):
                detail_data['building_area'] = line
                print(f"    ğŸ  {line}")
            
            if 'ç¯‰å¹´' in line or 'å»ºç¯‰å¹´' in line or ('å»ºç¯‰' in line and 'å¹´' in line):
                detail_data['built_year'] = line
                print(f"    ğŸ“… {line}")
        
        images = soup.find_all('img')
        for img in images:
            src = img.get('src', '')
            if any(keyword in src.lower() for keyword in ['photo', 'image', 'img', 'pic', 'akiya']):
                if not src.startswith('http'):
                    src = f"https://www.city.iida.lg.jp{src}"
                detail_data['images'].append(src)
        
        print(f"    ğŸ–¼ï¸  ç”»åƒ: {len(detail_data['images'])}æš")
        
        return detail_data
    
    def parse(self, html):
        """HTMLã‹ã‚‰ç‰©ä»¶æƒ…å ±ã‚’æŠ½å‡º"""
        soup = BeautifulSoup(html, 'html.parser')
        
        print("\n=== ç‰©ä»¶ã‚’æŠ½å‡ºä¸­ ===")
        
        links = soup.find_all('a', href=True)
        
        for link in links:
            text = link.text.strip()
            href = link.get('href', '')
            
            if 'ç©ºãå®¶æƒ…å ±' in text and 'ç‰©ä»¶ç•ªå·' in text and 'æˆç´„æ¸ˆ' not in text:
                print(f"\nç™ºè¦‹: {text[:60]}")
                
                property_match = re.search(r'ç‰©ä»¶ç•ªå·[ï¼š:]\s*([^\sã€]+)', text)
                property_id = property_match.group(1) if property_match else 'unknown'
                
                property_type = 'å£²è²·' if 'å£²è²·' in text else 'è³ƒè²¸' if 'è³ƒè²¸' in text else 'ä¸æ˜'
                
                if not href.startswith('http'):
                    full_url = f"https://www.city.iida.lg.jp{href}"
                else:
                    full_url = href
                
                property_data = {
                    'id': property_id,
                    'title': text,
                    'municipality': self.municipality_name,
                    'url': full_url,
                    'type': property_type,
                }
                
                detail_data = self.parse_detail_page(full_url, property_id)
                property_data.update(detail_data)
                
                self.properties.append(property_data)
                print(f"  âœ… {property_id} å®Œäº†")
        
        print(f"\n" + "="*50)
        print(f"åˆè¨ˆ: {len(self.properties)}ä»¶ã®ç‰©ä»¶ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
        print("="*50)
    
    def run(self):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ"""
        print(f"=== {self.municipality_name} ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ ===")
        
        html = self.fetch_page(self.base_url)
        if html:
            self.parse(html)
        
        output_path = '../../data/raw/iida.json'
        self.save_data(output_path)
        
        return self.properties

if __name__ == '__main__':
    scraper = IidaScraper()
    scraper.run()