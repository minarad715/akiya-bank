import sys
sys.path.append('../../')

from scrapers.base_scraper import BaseScraper
from bs4 import BeautifulSoup
import re

class HokutoScraper(BaseScraper):
    """å±±æ¢¨çœŒåŒ—æœå¸‚ã®ç©ºãå®¶ãƒãƒ³ã‚¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self):
        super().__init__(
            municipality_name="å±±æ¢¨çœŒåŒ—æœå¸‚",
            base_url="https://www.city.hokuto.yamanashi.jp/teijyu_ijyu/bank/"
        )
    
    def parse_detail_page(self, url, property_id):
        """ç‰©ä»¶è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’å–å¾—"""
        print(f"\n  è©³ç´°ãƒšãƒ¼ã‚¸å–å¾—: {property_id}")
        html = self.fetch_page(url)
        
        if not html:
            return {}
        
        soup = BeautifulSoup(html, 'html.parser')
        
        detail_data = {
            'price': None,
            'price_raw': None,
            'land_area': None,
            'building_area': None,
            'built_year': None,
            'description': '',
            'images': []
        }
        
        # ãƒšãƒ¼ã‚¸å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
        page_text = soup.get_text()
        
        for line in page_text.split('\n'):
            line = line.strip()
            
            # ä¾¡æ ¼
            if 'å£²å´ä¾¡æ ¼' in line or 'è³ƒæ–™' in line:
                detail_data['price_raw'] = line
                # æ•°å­—ã‚’æŠ½å‡ºï¼ˆä¾‹ï¼š380ä¸‡å†† â†’ 3800000ï¼‰
                price_match = re.search(r'(\d+)ä¸‡å††', line)
                if price_match:
                    detail_data['price'] = int(price_match.group(1)) * 10000
                print(f"    ğŸ’° {line}")
            
            # åœŸåœ°é¢ç©
            if 'åœŸåœ°ï¼š' in line:
                detail_data['land_area'] = line.replace('åœŸåœ°ï¼š', '').strip()
                print(f"    ğŸ“ {line}")
            
            # å»ºç‰©é¢ç©
            if 'å»ºç‰©ï¼š' in line:
                detail_data['building_area'] = line.replace('å»ºç‰©ï¼š', '').strip()
                print(f"    ğŸ  {line}")
            
            # ç¯‰å¹´
            if 'å»ºç¯‰å¹´ï¼š' in line:
                detail_data['built_year'] = line.replace('å»ºç¯‰å¹´ï¼š', '').strip()
                print(f"    ğŸ“… {line}")
        
        # ç”»åƒã‚’å–å¾—
        images = soup.find_all('img')
        for img in images:
            src = img.get('src', '')
            if '/fs/' in src:  # ç‰©ä»¶å†™çœŸ
                if not src.startswith('http'):
                    src = f"https://www.city.hokuto.yamanashi.jp{src}"
                detail_data['images'].append(src)
        
        print(f"    ğŸ–¼ï¸  ç”»åƒ: {len(detail_data['images'])}æš")
        
        return detail_data
    
    def parse(self, html):
        """HTMLã‹ã‚‰ç‰©ä»¶æƒ…å ±ã‚’æŠ½å‡º"""
        soup = BeautifulSoup(html, 'html.parser')
        
        print("\n=== ç‰©ä»¶ã‚’æŠ½å‡ºä¸­ ===")
        property_headings = soup.find_all('h3')
        
        for heading in property_headings:
            heading_text = heading.text.strip()
            
            if 'ç©ºãå®¶ãƒãƒ³ã‚¯ç™»éŒ²ç‰©ä»¶' in heading_text:
                print(f"\nç™ºè¦‹: {heading_text}")
                
                property_number = re.search(r'ã€(.*?)ã€‘', heading_text)
                property_id = property_number.group(1) if property_number else 'unknown'
                
                parent = heading.parent
                if parent and parent.name == 'a':
                    relative_url = parent.get('href')
                    full_url = f"https://www.city.hokuto.yamanashi.jp{relative_url}"
                    
                    property_data = {
                        'id': property_id,
                        'title': heading_text,
                        'municipality': self.municipality_name,
                        'url': full_url,
                        'type': 'å£²å´' if 'å£²' in property_id else 'è³ƒè²¸',
                    }
                    
                    detail_data = self.parse_detail_page(full_url, property_id)
                    property_data.update(detail_data)
                    
                    self.properties.append(property_data)
                    print(f"  âœ… {property_id} å®Œäº†")
        
        print(f"\n" + "="*50)
        print(f"åˆè¨ˆ: {len(self.properties)}ä»¶ã®ç‰©ä»¶ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
        print("="*50)
    
    def run(self):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ"""
        print(f"=== {self.municipality_name} ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ ===")
        
        html = self.fetch_page(self.base_url)
        if html:
            self.parse(html)
        
        output_path = '../../data/raw/hokuto.json'
        self.save_data(output_path)
        
        return self.properties

if __name__ == '__main__':
    scraper = HokutoScraper()
    scraper.run()