import sys
from base_scraper import BaseScraper
from bs4 import BeautifulSoup
import re

class AthomeOkinawaScraper(BaseScraper):
    """ã‚¢ãƒƒãƒˆãƒ›ãƒ¼ãƒ æ²–ç¸„ã®ç©ºãå®¶ãƒãƒ³ã‚¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self):
        super().__init__(
            municipality_name="æ²–ç¸„çœŒï¼ˆã‚¢ãƒƒãƒˆãƒ›ãƒ¼ãƒ ï¼‰",
            base_url="https://www.akiya-athome.jp/buy/47/"
        )
    
    def parse_detail_page(self, url, property_id):
        """ç‰©ä»¶è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’å–å¾—"""
        print(f"\n  è©³ç´°ãƒšãƒ¼ã‚¸å–å¾—: {property_id}")
        html = self.fetch_page(url)
        
        if not html:
            return {}
        
        soup = BeautifulSoup(html, 'html.parser')
        
        detail_data = {
            'land_area': None,
            'building_area': None,
            'built_year': None,
            'description': '',
            'images': []
        }
        
        page_text = soup.get_text()
        
        for line in page_text.split('\n'):
            line = line.strip()
            
            if 'åœŸåœ°é¢ç©' in line or ('åœŸåœ°' in line and ('ã¡' in line or 'åª' in line)):
                detail_data['land_area'] = line
                print(f"    ğŸ“ {line[:60]}")
            
            if 'å»ºç‰©é¢ç©' in line or ('å»ºç‰©' in line and ('ã¡' in line or 'åª' in line)):
                detail_data['building_area'] = line
                print(f"    ğŸ  {line[:60]}")
            
            if 'ç¯‰å¹´' in line or 'å»ºç¯‰å¹´' in line:
                detail_data['built_year'] = line
                print(f"    ğŸ“… {line[:60]}")
        
        # ç”»åƒã‚’å–å¾—
        images = soup.find_all('img')
        for img in images:
            src = img.get('src', '')
            if src and not any(skip in src.lower() for skip in ['logo', 'icon', 'banner', 'btn', 'button']):
                if src.startswith('//'):
                    src = 'https:' + src
                elif src.startswith('/'):
                    base_domain = re.search(r'https://[^/]+', url).group(0)
                    src = f"{base_domain}{src}"
                elif not src.startswith('http'):
                    base_domain = re.search(r'https://[^/]+', url).group(0)
                    src = f"{base_domain}/{src}"
                
                if src not in detail_data['images']:
                    detail_data['images'].append(src)
        
        print(f"    ğŸ–¼ï¸  ç”»åƒ: {len(detail_data['images'])}æš")
        
        return detail_data
    
    def parse(self, html):
        """HTMLã‹ã‚‰ç‰©ä»¶æƒ…å ±ã‚’æŠ½å‡º"""
        soup = BeautifulSoup(html, 'html.parser')
        
        print("\n=== ç‰©ä»¶ã‚’æŠ½å‡ºä¸­ ===")
        
        links = soup.find_all('a', href=True)
        
        processed_urls = set()
        
        for link in links:
            href = link.get('href', '')
            
            if '/bukken/detail/buy/' in href:
                if href in processed_urls:
                    continue
                processed_urls.add(href)
                
                property_id_match = re.search(r'/buy/(\d+)', href)
                property_id = property_id_match.group(1) if property_id_match else 'unknown'
                
                municipality_match = re.search(r'https://([^.]+)\.akiya-athome\.jp', href)
                municipality_code = municipality_match.group(1) if municipality_match else 'unknown'
                
                title = link.text.strip()
                if not title or title == 'â€» è©³ç´°ã‚’è¦‹ã‚‹':
                    parent = link.parent
                    if parent:
                        title = parent.get_text(strip=True).replace('â€» è©³ç´°ã‚’è¦‹ã‚‹', '')[:100]
                
                print(f"\nç™ºè¦‹: {title[:60]} (ID: {property_id})")
                
                # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ä¾¡æ ¼ã‚’æŠ½å‡º
                price = None
                price_raw = None
                price_match = re.search(r'ä¾¡æ ¼(\d+(?:,\d+)?)ä¸‡å††', title)
                if price_match:
                    price_str = price_match.group(1).replace(',', '')
                    price = int(price_str) * 10000
                    price_raw = f"ä¾¡æ ¼{price_match.group(1)}ä¸‡å††"
                    print(f"  ğŸ’° {price_raw}")
                
                property_data = {
                    'id': property_id,
                    'title': title or f"ç‰©ä»¶ç•ªå·{property_id}",
                    'municipality': f"æ²–ç¸„çœŒ{municipality_code}",
                    'url': href,
                    'type': 'å£²è²·',
                    'price': price,
                    'price_raw': price_raw,
                }
                
                detail_data = self.parse_detail_page(href, property_id)
                property_data.update(detail_data)
                
                self.properties.append(property_data)
                print(f"  âœ… {property_id} å®Œäº†")
        
        print(f"\n" + "="*50)
        print(f"åˆè¨ˆ: {len(self.properties)}ä»¶ã®ç‰©ä»¶ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
        print("="*50)
    
    def run(self):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ"""
        print(f"=== {self.municipality_name} ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ ===")
        
        html = self.fetch_page(self.base_url)
        if html:
            self.parse(html)
        
        output_path = '../data/raw/athome_okinawa.json'
        self.save_data(output_path)
        
        return self.properties

if __name__ == '__main__':
    scraper = AthomeOkinawaScraper()
    scraper.run()